{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required Libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading & Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data in pandas\n",
    "inp0= pd.read_csv(\"Attribute+DataSet.csv \")\n",
    "inp1= pd.read_csv(\"Dress+Sales.csv \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have “Attribute DataSet” which contains a column named “Price”. Choose the correct statement from the following about its data type and variable type.\n",
    "- Integer type and numerical variable\n",
    "- Object type and categorical ordinal variable\n",
    "- Object type and categorical nominal variable\n",
    "- Float type and categorical variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another column in “Attribute DataSet” named as “Recommendation”, choose the correct statement about its data type and variable type.\n",
    "- Integer type and categorical\n",
    "- Object type and categorical\n",
    "- Integer type and continuous numerical\n",
    "- Object type only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following column do you think are of no use in “Attribute DataSet”.\n",
    "- Dress_ID\n",
    "- Price\n",
    "- Size and material\n",
    "- NeckLine\n",
    "- None of the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 479 entries, 0 to 478\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Dress_ID        479 non-null    int64  \n",
      " 1   Style           479 non-null    object \n",
      " 2   Price           477 non-null    object \n",
      " 3   Rating          479 non-null    float64\n",
      " 4   Size            479 non-null    object \n",
      " 5   Season          477 non-null    object \n",
      " 6   NeckLine        476 non-null    object \n",
      " 7   SleeveLength    477 non-null    object \n",
      " 8   Material        360 non-null    object \n",
      " 9   FabricType      223 non-null    object \n",
      " 10  Decoration      255 non-null    object \n",
      " 11  Pattern Type    377 non-null    object \n",
      " 12  Recommendation  479 non-null    int64  \n",
      "dtypes: float64(1), int64(2), object(10)\n",
      "memory usage: 48.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 479 entries, 0 to 478\n",
      "Data columns (total 24 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Dress_ID    479 non-null    float64\n",
      " 1   29-08-2013  479 non-null    int64  \n",
      " 2   31-08-2013  479 non-null    int64  \n",
      " 3   09-02-2013  479 non-null    int64  \n",
      " 4   09-04-2013  479 non-null    int64  \n",
      " 5   09-06-2013  479 non-null    int64  \n",
      " 6   09-08-2013  479 non-null    int64  \n",
      " 7   09-10-2013  479 non-null    int64  \n",
      " 8   09-12-2013  479 non-null    object \n",
      " 9   14-09-2013  479 non-null    object \n",
      " 10  16-09-2013  479 non-null    object \n",
      " 11  18-09-2013  479 non-null    object \n",
      " 12  20-09-2013  479 non-null    object \n",
      " 13  22-09-2013  479 non-null    object \n",
      " 14  24-09-2013  479 non-null    int64  \n",
      " 15  26-09-2013  257 non-null    float64\n",
      " 16  28-09-2013  479 non-null    int64  \n",
      " 17  30-09-2013  222 non-null    float64\n",
      " 18  10-02-2013  220 non-null    float64\n",
      " 19  10-04-2013  221 non-null    float64\n",
      " 20  10-06-2013  479 non-null    int64  \n",
      " 21  10-08-2013  224 non-null    float64\n",
      " 22  10-10-2013  224 non-null    float64\n",
      " 23  10-12-2013  479 non-null    int64  \n",
      "dtypes: float64(7), int64(11), object(6)\n",
      "memory usage: 89.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the information about the attributes of inp0 and inp1.\n",
    "print(inp0.info())\n",
    "print(inp1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the Rows and Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a column in “Attribute Dataset” named as ‘Size’. This column contains the values in abbreviation format. Write a code in Python to convert the followings:\n",
    "\n",
    "- M into  “Medium”\n",
    "- L into  “Large”\n",
    "- XL into “Extra large”\n",
    "- free into “Free”\n",
    "- S, s & small into “Small”.\n",
    "\n",
    "Now once you are done with changes in the dataset, what is the value of the lowest percentage, the highest percentage and the percentage of Small size categories in the column named “Size”?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Percentage: 4.45859872611465\n",
      "Highest Percentage: 54.45859872611465\n",
      "Percentage of Small Size Categories: 11.464968152866243\n"
     ]
    }
   ],
   "source": [
    "# Column fixing, correcting size abbreviation. count the percentage of each size category in \"Size\" column.\n",
    "size_mapping = {\n",
    "    'M': 'Medium',\n",
    "    'L': 'Large',\n",
    "    'XL': 'Extra large',\n",
    "    'Free': 'Free',\n",
    "    'S': 'Small',\n",
    "    's': 'Small',\n",
    "    'small': 'Small'\n",
    "}\n",
    "\n",
    "# Convert 'Size' column values using the mapping dictionary\n",
    "inp0['Size'] = inp0['Size'].map(size_mapping)\n",
    "\n",
    "# Calculate the percentage of each size category\n",
    "size_percentages = inp0['Size'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Find the lowest, highest, and percentage of small size categories\n",
    "lowest_percentage = size_percentages.min()\n",
    "highest_percentage = size_percentages.max()\n",
    "percentage_small = size_percentages['Small']\n",
    "\n",
    "print(\"Lowest Percentage:\", lowest_percentage)\n",
    "print(\"Highest Percentage:\", highest_percentage)\n",
    "print(\"Percentage of Small Size Categories:\", percentage_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Medium         0.544586\n",
       "Large          0.296178\n",
       "Small          0.114650\n",
       "Extra large    0.044586\n",
       "Name: Size, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the value counts of each category in \"Size\" column.\n",
    "inp0.Size= inp0.Size.replace(['S', 'small', 's'], \"Small\")\n",
    "\n",
    "inp0.Size= inp0.Size.replace('free', \"Free\")\n",
    "\n",
    "inp0.Size= inp0.Size.replace('M', \"Medium\")\n",
    "\n",
    "inp0.Size= inp0.Size.replace('L', \"Large\") \n",
    "\n",
    "inp0.Size= inp0.Size.replace('XL', \"Extra large\")\n",
    "\n",
    "inp0.Size.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute/Remove Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the null count of each variables of inp0 and inp1.\n",
    "inp1.loc[inp1['09-12-2013']== 'Removed',\"09-12-2013\"] = np.NaN\n",
    "\n",
    "inp1.loc[inp1['14-09-2013']== 'removed',\"14-09-2013\"] = np.NaN\n",
    "\n",
    "inp1.loc[inp1['16-09-2013']== 'removed',\"16-09-2013\"] = np.NaN\n",
    "\n",
    "inp1.loc[inp1['18-09-2013']== 'removed',\"18-09-2013\"] = np.NaN\n",
    "\n",
    "inp1.loc[inp1['20-09-2013']== 'removed',\"20-09-2013\"] = np.NaN\n",
    "\n",
    "inp1.loc[inp1['22-09-2013']== 'Orders',\"22-09-2013\"] = np.NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given another dataset named “Dress Sales”. Now if you observe the datatypes of the columns using ‘inp1.info()’ command, you can identify that there are certain columns defined as object data type though they primarily consist of numeric data.\n",
    "\n",
    "Now if you try and convert these object data type columns into numeric data type(float), you will come across an error message. Try to correct this error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress_ID      float64\n",
      "29-08-2013      int64\n",
      "31-08-2013      int64\n",
      "09-02-2013      int64\n",
      "09-04-2013      int64\n",
      "09-06-2013      int64\n",
      "09-08-2013      int64\n",
      "09-10-2013      int64\n",
      "09-12-2013     object\n",
      "14-09-2013     object\n",
      "16-09-2013     object\n",
      "18-09-2013     object\n",
      "20-09-2013     object\n",
      "22-09-2013     object\n",
      "24-09-2013      int64\n",
      "26-09-2013    float64\n",
      "28-09-2013      int64\n",
      "30-09-2013    float64\n",
      "10-02-2013    float64\n",
      "10-04-2013    float64\n",
      "10-06-2013      int64\n",
      "10-08-2013    float64\n",
      "10-10-2013    float64\n",
      "10-12-2013      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the data types information of inp1 i.e. \"Dress Sales\" data.\n",
    "print(inp1.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dress_ID</th>\n",
       "      <th>29-08-2013</th>\n",
       "      <th>31-08-2013</th>\n",
       "      <th>09-02-2013</th>\n",
       "      <th>09-04-2013</th>\n",
       "      <th>09-06-2013</th>\n",
       "      <th>09-08-2013</th>\n",
       "      <th>09-10-2013</th>\n",
       "      <th>09-12-2013</th>\n",
       "      <th>14-09-2013</th>\n",
       "      <th>...</th>\n",
       "      <th>24-09-2013</th>\n",
       "      <th>26-09-2013</th>\n",
       "      <th>28-09-2013</th>\n",
       "      <th>30-09-2013</th>\n",
       "      <th>10-02-2013</th>\n",
       "      <th>10-04-2013</th>\n",
       "      <th>10-06-2013</th>\n",
       "      <th>10-08-2013</th>\n",
       "      <th>10-10-2013</th>\n",
       "      <th>10-12-2013</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.006033e+09</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>2274.0</td>\n",
       "      <td>2491.0</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>2727.0</td>\n",
       "      <td>2887.0</td>\n",
       "      <td>2930.0</td>\n",
       "      <td>3119.0</td>\n",
       "      <td>3204.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3554.0</td>\n",
       "      <td>3624.0</td>\n",
       "      <td>3706.0</td>\n",
       "      <td>3746.0</td>\n",
       "      <td>3795.0</td>\n",
       "      <td>3832.0</td>\n",
       "      <td>3897.0</td>\n",
       "      <td>3923.0</td>\n",
       "      <td>3985.0</td>\n",
       "      <td>4048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.212192e+09</td>\n",
       "      <td>151.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>1558.0</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>2942.0</td>\n",
       "      <td>3258.0</td>\n",
       "      <td>3354.0</td>\n",
       "      <td>3475.0</td>\n",
       "      <td>3654.0</td>\n",
       "      <td>3911.0</td>\n",
       "      <td>4024.0</td>\n",
       "      <td>4125.0</td>\n",
       "      <td>4277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.190381e+09</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.660060e+08</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>1914.0</td>\n",
       "      <td>1924.0</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.763395e+08</td>\n",
       "      <td>996.0</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>1432.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2032.0</td>\n",
       "      <td>2156.0</td>\n",
       "      <td>2252.0</td>\n",
       "      <td>2312.0</td>\n",
       "      <td>2387.0</td>\n",
       "      <td>2459.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>2614.0</td>\n",
       "      <td>2693.0</td>\n",
       "      <td>2736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>9.905592e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>7.133920e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>5.328743e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>...</td>\n",
       "      <td>616.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>6.554649e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>257.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>9.199310e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>...</td>\n",
       "      <td>628.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>668.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dress_ID  29-08-2013  31-08-2013  09-02-2013  09-04-2013  09-06-2013  \\\n",
       "0    1.006033e+09      2114.0      2274.0      2491.0      2660.0      2727.0   \n",
       "1    1.212192e+09       151.0       275.0       570.0       750.0       813.0   \n",
       "2    1.190381e+09         6.0         7.0         7.0         7.0         8.0   \n",
       "3    9.660060e+08      1005.0      1128.0      1326.0      1455.0      1507.0   \n",
       "4    8.763395e+08       996.0      1175.0      1304.0      1396.0      1432.0   \n",
       "..            ...         ...         ...         ...         ...         ...   \n",
       "474  9.905592e+08         0.0         0.0         0.0        60.0        62.0   \n",
       "475  7.133920e+08         0.0         0.0         0.0       560.0       554.0   \n",
       "476  5.328743e+08         0.0         0.0         0.0       734.0       728.0   \n",
       "477  6.554649e+08         0.0         0.0         0.0       254.0       259.0   \n",
       "478  9.199310e+08         0.0         0.0         0.0       538.0       545.0   \n",
       "\n",
       "     09-08-2013  09-10-2013  09-12-2013  14-09-2013  ...  24-09-2013  \\\n",
       "0        2887.0      2930.0      3119.0      3204.0  ...      3554.0   \n",
       "1        1066.0      1164.0      1558.0      1756.0  ...      2710.0   \n",
       "2           8.0         9.0        10.0        10.0  ...        11.0   \n",
       "3        1621.0      1637.0      1723.0      1746.0  ...      1878.0   \n",
       "4        1559.0      1570.0      1638.0      1655.0  ...      2032.0   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "474        64.0        65.0        67.0        68.0  ...        73.0   \n",
       "475       544.0       537.0       525.0       519.0  ...       400.0   \n",
       "476       726.0       715.0       694.0       690.0  ...       616.0   \n",
       "477       261.0       263.0       268.0       270.0  ...       257.0   \n",
       "478       558.0       563.0       578.0       585.0  ...       628.0   \n",
       "\n",
       "     26-09-2013  28-09-2013  30-09-2013  10-02-2013  10-04-2013  10-06-2013  \\\n",
       "0        3624.0      3706.0      3746.0      3795.0      3832.0      3897.0   \n",
       "1        2942.0      3258.0      3354.0      3475.0      3654.0      3911.0   \n",
       "2          11.0        11.0        11.0        11.0        11.0        11.0   \n",
       "3        1892.0      1914.0      1924.0      1929.0      1941.0      1952.0   \n",
       "4        2156.0      2252.0      2312.0      2387.0      2459.0      2544.0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "474        74.0        75.0        75.0        76.0        76.0        77.0   \n",
       "475       388.0       360.0       364.0       372.0       377.0       380.0   \n",
       "476       597.0       586.0       569.0       561.0       555.0       551.0   \n",
       "477       256.0       255.0       254.0       253.0       250.0       249.0   \n",
       "478       632.0       639.0       645.0       651.0       655.0       660.0   \n",
       "\n",
       "     10-08-2013  10-10-2013  10-12-2013  \n",
       "0        3923.0      3985.0      4048.0  \n",
       "1        4024.0      4125.0      4277.0  \n",
       "2          11.0        11.0        11.0  \n",
       "3        1955.0      1959.0      1963.0  \n",
       "4        2614.0      2693.0      2736.0  \n",
       "..          ...         ...         ...  \n",
       "474        77.0        77.0        77.0  \n",
       "475       382.0       384.0       285.0  \n",
       "476       546.0       535.0       520.0  \n",
       "477       249.0       249.0       248.0  \n",
       "478       668.0       674.0       680.0  \n",
       "\n",
       "[479 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to convert the object type into float type of data. YOU GET ERROR MESSAGE.\n",
    "inp1 = inp1.astype(float)\n",
    "inp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress_ID            0\n",
      "Style               0\n",
      "Price               2\n",
      "Rating              0\n",
      "Size              165\n",
      "Season              2\n",
      "NeckLine            3\n",
      "SleeveLength        2\n",
      "Material          119\n",
      "FabricType        256\n",
      "Decoration        224\n",
      "Pattern Type      102\n",
      "Recommendation      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Do the required changes in the \"Dress Sales\" data set to get null values on string values.\n",
    "inp0 = inp0.replace('string_value', np.nan)\n",
    "\n",
    "# Check for null values\n",
    "print(inp0.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress_ID      float64\n",
      "29-08-2013    float64\n",
      "31-08-2013    float64\n",
      "09-02-2013    float64\n",
      "09-04-2013    float64\n",
      "09-06-2013    float64\n",
      "09-08-2013    float64\n",
      "09-10-2013    float64\n",
      "09-12-2013    float64\n",
      "14-09-2013    float64\n",
      "16-09-2013    float64\n",
      "18-09-2013    float64\n",
      "20-09-2013    float64\n",
      "22-09-2013    float64\n",
      "24-09-2013    float64\n",
      "26-09-2013    float64\n",
      "28-09-2013    float64\n",
      "30-09-2013    float64\n",
      "10-02-2013    float64\n",
      "10-04-2013    float64\n",
      "10-06-2013    float64\n",
      "10-08-2013    float64\n",
      "10-10-2013    float64\n",
      "10-12-2013    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the object type columns in \"Dress Sales\" into float type of data type.\n",
    "dates = ['29-08-2013', '31-08-2013', '09-02-2013', '09-04-2013', '09-06-2013', '09-08-2013', '09-10-2013',\n",
    "         '09-12-2013', '14-09-2013', '16-09-2013', '18-09-2013', '20-09-2013', '22-09-2013', '24-09-2013',\n",
    "         '26-09-2013', '28-09-2013', '30-09-2013', '10-02-2013', '10-04-2013', '10-06-2013', '10-08-2013',\n",
    "         '10-10-2013', '10-12-2013']\n",
    "\n",
    "# Repeat for other columns\n",
    "for i in dates:\n",
    "    inp1[i] = inp1[i].astype(float)\n",
    "# Check the data types\n",
    "print(inp1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see the null counts in “Dress Sales” dataset after performing all the operations that have been mentioned in jupyter notebook, you will find that there are some columns in “Dress Sales” data where there are more than 40% of missing values. Based on your understanding of dealing with missing values do the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the null percetange of each column of inp1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns in \"Dress Sales\" which have more than 40% of missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should categorise the dates into seasons in “Dress Sales” data to simplify the analysis according to the following criteria:\n",
    "- June, July and August: Summer.\n",
    "- September, October and November: Autumn.\n",
    "- December, January and February: WInter.\n",
    "- March, April and May: Spring.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'09-04-2013'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:162\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:203\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:211\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '09-04-2013'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpring\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43minp1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m09-04-2013\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m29-08-2013\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m31-08-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-06-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-08-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10-06-2013\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWinter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-02-2013\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-12-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10-12-2013\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    911\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpring\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m09-04-2013\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m29-08-2013\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m31-08-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-06-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-08-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10-06-2013\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWinter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inp1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-02-2013\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09-12-2013\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10-12-2013\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '09-04-2013'"
     ]
    }
   ],
   "source": [
    "inp1['Spring'] = inp1.apply(lambda x: x['09-04-2013'], axis=1)\n",
    "\n",
    "inp1['Summer'] = inp1.apply(lambda x: x['29-08-2013'] + x['31-08-2013']+ x['09-06-2013']+ x['09-08-2013']+ x['10-06-2013'], axis=1)\n",
    "\n",
    "inp1['Winter'] = inp1.apply(lambda x: x['09-02-2013'] + x['09-12-2013']+ x['10-12-2013'], axis=1)\n",
    "\n",
    "inp1['Autumn'] = inp1.apply(lambda x: x['09-10-2013'] + x['14-09-2013']+ x['16-09-2013']+ x['18-09-2013']+ x['20-09-2013']+ x['22-09-2013']+ x['24-09-2013']+ x['28-09-2013'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum of sales in each seasons in inp1 i.e. \"Dress Sales\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge inp1 with inp0 with left join manner, so that the information of inp0 should remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inp0 with inp1 into inp0. this is also called left merge.\n",
    "inp0 = pd.merge(left=inp0,right=inp1, how='left', left_on='Dress_ID', right_on='Dress_ID')\n",
    "inp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Drop the Date columns from inp0 as it is already combined into four seasons.\n",
    "inp0.drop(inp0.loc[:,'29-08-2013':'10-12-2013'].columns, axis= 1, inplace= True)\n",
    "inp0.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the null count of inp0 to get the idea about the missing values in data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the null count of each columns in inp0 dataframe i.e. combined data frame of inp0 and inp1 without date columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are two types of variables one with a large number of missing values and another is very less number of missing values. These two columns can be categorized as:\n",
    "\n",
    "Type-1: Missing values are very less (around 2 or 3 missing values): Price, Season, NeckLine, SleeveLength, Winter and Autumn. \n",
    "\n",
    "Type-2: Missing values are large in numbers (more than 15%): Material, FabricType, Decoration and Pattern Type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with the missing values of Type-1 columns: Price, Season, NeckLine, SleeveLength, Winter and Autumn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with the missing values for Type-2 columns: Material, FabricType, Decoration and Pattern Type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given dataset, there are certain discrepancies with the categorical names such as irregular spellings. Choose the correct option of columns with irregular categories and update them.\n",
    " \n",
    "- Season, NeckLine\n",
    "- Price, Material\n",
    "- fabricType, Decoration\n",
    "- Season, SleeveLength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting the spellings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting the Spellings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Unordered Univariate Analysis\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a column named ‘Style’ in ‘Attribute Dataset’ which consists of the different style categories of the women apparels. Certain categories whose total sale is less than 50000 across all the seasons is considered under one single category as ‘Others’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following categories in ‘Style’ column can be grouped into ‘Others’ category? and perform the grouping operation in the notebook for further analysis.\n",
    "- Flare, fashion\n",
    "- Novelty, bohemian\n",
    "- OL, fashion, work\n",
    "- Novelty, fashion, Flare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"Style\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of “cute” and “Others” category in “Style” column in “Attribute DataSet” respectively?\n",
    "- 46%, 5%\n",
    "- 9%, 2.1%\n",
    "- 2.1%, 5%\n",
    "- 13.8%, 9%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each categories in the \"Style\" variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly Club Neckline, SLeeve length categories into \"Others\" which have less than 50000 sales across all the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"Neckline\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"Sleeve length\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Club material, fabrictype, patterntype and decoration categories into \"Others\" which have less than 25000 sales across all the seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"material\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"fabric type\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"patern type\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"decoration\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caregorical Ordered Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following is an unordered variable in “Attribute DataSet”.\n",
    "- Style\n",
    "- Price\n",
    "- Season\n",
    "- Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variable Univariate analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the approximate difference between the maximum value and 75th percentile in “Autumn” column.\n",
    "- Approx 54000\n",
    "- Approx 55000\n",
    "- Approx 52000\n",
    "- Approx 50000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the numerical variale: \"Autumn\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot of \"Autumn\" column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following season has the highest difference between the maximum value and 99th quantile of sales?\n",
    "- Winter\n",
    "- Summer\n",
    "- Spring\n",
    "- Autumn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum and 99th percentile of Winter season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum and 99th percentile of Summer season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum and 99th percentile of Spring season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum and 99th percentile of Autumn season.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical- Categorical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following “Price” category has the lowest average value of rating?\n",
    "- very-high\n",
    "- Medium\n",
    "- Low\n",
    "- High\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Mean of Ratings for each Price category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the median of the rating of “vintage” category in Style column?\n",
    "- 4.6\n",
    "- 4.7\n",
    "- 4.55\n",
    "- 0.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the median of Ratings for each Style category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following season has the highest average value of sale for “Recommendation” value equals to 1.\n",
    "- Summer\n",
    "- Spring\n",
    "- Autumn\n",
    "- Winter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summer sale vs Recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spring sale vs Recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autumn sale vs Recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winter sale vs Recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical categorical bivariate analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following size categories has the highest positive recommendations?\n",
    "- Medium and extra large\n",
    "- Extra large and small\n",
    "- Free and small\n",
    "- Free and medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size vs Recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following pair of “Style” and “Price” category has the highest average of positive recommendations?\n",
    "- Price: medium and style: vintage\n",
    "- Price: medium and style: cute\n",
    "- Price: very high and style: party\n",
    "- Price: low and style: sexy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heat map of Style, price and Recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following material type has no recommendation in summer and winter seasons?\n",
    "- Mix and Milksilk\n",
    "- Nylon and Rayon\n",
    "- Microfiber and Silk\n",
    "- Milksilk and Microfiber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heat map of Season, material and Recommendation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
